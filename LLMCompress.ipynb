{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8b583b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nyrarodrigues/Desktop/Fall 2025/RAGLMCompress/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import Iterator\n",
    "from arithmetic_coder import arithmetic_coder, ac_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43be0be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "class Metric:\n",
    "    def __init__(self):\n",
    "        self.total_length = 0\n",
    "        self.compressed_length = 0\n",
    "\n",
    "    def compute_ratio(self):\n",
    "        if self.total_length != 0 and self.compressed_length != 0:\n",
    "            return (\n",
    "                self.total_length / self.compressed_length,\n",
    "                self.compressed_length / self.total_length,\n",
    "            )\n",
    "        else:\n",
    "            return 0, 0\n",
    "\n",
    "    def accumulate(self, compressed, original):\n",
    "        if isinstance(compressed, list):\n",
    "            self.compressed_length += len(compressed)\n",
    "        elif isinstance(compressed, int):\n",
    "            self.compressed_length += compressed\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported compressed length type: {type(compressed)}\")\n",
    "\n",
    "        if isinstance(original, list):\n",
    "            self.total_length += len(original)\n",
    "        elif isinstance(original, int):\n",
    "            self.total_length += original\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported original length type: {type(original)}\")\n",
    "\n",
    "\n",
    "def compress(compress_input, logits, metric):\n",
    "    \"\"\"\n",
    "    :param compress_input: symbols to be compressed\n",
    "    :param logits: generation probabilities from the model\n",
    "    :param metric: compression metrics\n",
    "    :return: compressed result, a floating number\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    # Initialize a Encoder Object\n",
    "    # Precision is for the encoder, not the model\n",
    "    # You must have the same precision for encoder and decoder\n",
    "    # Tricky things here: Though theoratically prefill == decode, but in practice there are numerical problems\n",
    "    encoder = arithmetic_coder.Encoder(\n",
    "        base=2,\n",
    "        precision=64,\n",
    "        output_fn=output.append,\n",
    "    )\n",
    "    # the first symbol should be saved for generation in decoding\n",
    "    start_symbol = compress_input[:, :1]\n",
    "    probs = logits.softmax(dim=-1).to(torch.float32)\n",
    "    pd = torch.gather(probs, dim=-1, index=compress_input[:, 1:].unsqueeze(-1)).squeeze(\n",
    "        -1\n",
    "    )\n",
    "\n",
    "    probs = np.vstack(probs.detach().cpu().numpy().squeeze())\n",
    "\n",
    "    sequence_array = compress_input[:, 1:].detach().cpu().numpy().reshape(-1)\n",
    "\n",
    "    pd = pd.squeeze()\n",
    "\n",
    "    # compress the sequence\n",
    "    for symbol, prob, pd_prob in zip(sequence_array, probs, pd):\n",
    "        encoder.encode(\n",
    "            ac_utils.normalize_pdf_for_arithmetic_coding(prob, np.float32), symbol\n",
    "        )\n",
    "    encoder.terminate()\n",
    "\n",
    "    # to visualize and compute metrics, map to str\n",
    "    compressed_bits = \"\".join(map(str, output))\n",
    "    # you can only save in bytes, so need to pad some bits\n",
    "    compressed_bytes, num_padded_bits = ac_utils.bits_to_bytes(compressed_bits)\n",
    "    metric.accumulate(len(compressed_bytes) + num_padded_bits, len(sequence_array))\n",
    "\n",
    "    compress_rate, compress_ratio = metric.compute_ratio()\n",
    "    logger.info(f\"compressed length: {metric.compressed_length}\")\n",
    "    logger.info(f\"original length: {metric.total_length}\")\n",
    "    logger.info(f\"compression ratio: {compress_ratio:.6f}\")\n",
    "    logger.info(f\"compression rate: {compress_rate:.6f}\")\n",
    "\n",
    "    return compressed_bytes, num_padded_bits, start_symbol, sequence_array, pd, probs\n",
    "\n",
    "\n",
    "def decode(\n",
    "    compressed_bytes,\n",
    "    num_padded_bits,\n",
    "    model,\n",
    "    start_symbol,\n",
    "    device,\n",
    "    original_seq_len,\n",
    "    original_sequence=None,\n",
    "    pd=None,\n",
    "    probs=None,\n",
    "    do_test=True,\n",
    "):\n",
    "    \"\"\"\n",
    "\n",
    "    :param compressed_bytes:  compressed data\n",
    "    :param num_padded_bits:  padded bits\n",
    "    :param model: same model as encoder\n",
    "    :param start_symbol: first symbol to generate\n",
    "    :param original_sequence: original symbol sequence, for testing purpose\n",
    "    :param pd: actually not needed, used for testing\n",
    "    :param probs:\n",
    "    :param device:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # convert bytes back to bit stream\n",
    "    data_iter = iter(\n",
    "        ac_utils.bytes_to_bits(compressed_bytes, num_padded_bits=num_padded_bits)\n",
    "    )\n",
    "\n",
    "    # utils function to read bits\n",
    "    def _input_fn(bit_sequence: Iterator[str] = data_iter) -> int | None:\n",
    "        try:\n",
    "            return int(next(bit_sequence))\n",
    "        except StopIteration:\n",
    "            return None\n",
    "\n",
    "    # initialize a Decoder Object\n",
    "    decoder = arithmetic_coder.Decoder(\n",
    "        base=2,\n",
    "        precision=64,\n",
    "        input_fn=_input_fn,\n",
    "    )\n",
    "\n",
    "    sequence_array_de = start_symbol.squeeze(0).detach().cpu().numpy()\n",
    "    sequence_array_de_input = start_symbol\n",
    "    target_diff_list = []\n",
    "    target_in_top5_list = []\n",
    "\n",
    "    # loop for decompressing\n",
    "    # pad the input to the original length\n",
    "    sequence_array_de_input = torch.tensor(sequence_array_de_input, dtype=torch.long, device=device)\n",
    "    sequence_array_de_input = torch.nn.functional.pad(sequence_array_de_input, (0, original_seq_len-1), value=0)\n",
    "\n",
    "    for i in range(original_seq_len):\n",
    "        # attention_mask = (sequence_array_de_input != 0).long()\n",
    "        with torch.no_grad():\n",
    "            logits = model(sequence_array_de_input, use_cache=False).logits.to(\n",
    "                torch.float32\n",
    "            )\n",
    "        # get generaton probabilities, decode the next token\n",
    "        prob_de = logits.softmax(dim=-1).detach().cpu().numpy().squeeze(0)\n",
    "\n",
    "        de_token = decoder.decode(\n",
    "            ac_utils.normalize_pdf_for_arithmetic_coding(prob_de[i], np.float32)\n",
    "        )\n",
    "        # using the original probs to decode, for testing purpose\n",
    "        # de_token = decoder.decode(ac_utils.normalize_pdf_for_arithmetic_coding(probs[i]))\n",
    "        # append to the generated sequence\n",
    "        sequence_array_de = np.append(sequence_array_de, de_token)\n",
    "\n",
    "        current_len = len(sequence_array_de)\n",
    "        target_len = original_seq_len\n",
    "\n",
    "        if current_len < target_len:\n",
    "            padded = np.pad(\n",
    "                sequence_array_de, (0, (target_len - current_len)), constant_values=0\n",
    "            )\n",
    "        else:\n",
    "            padded = sequence_array_de\n",
    "        sequence_array_de_input = torch.tensor(\n",
    "            padded, dtype=torch.long, device=device\n",
    "        ).unsqueeze(0)\n",
    "\n",
    "        if do_test:\n",
    "            top_indices_de = prob_de[i].argsort()[-5:][::-1]\n",
    "            top_indices = probs[i].argsort()[-5:][::-1]\n",
    "\n",
    "            # target diff\n",
    "            target_diff = probs[i, original_sequence[i]] - prob_de[i, original_sequence[i]]\n",
    "            target_diff_list.append(target_diff)\n",
    "\n",
    "            # target in top 5\n",
    "            target_in_top5 = original_sequence[i] in top_indices\n",
    "            target_in_top5_list.append(target_in_top5)\n",
    "            print(\n",
    "                f\"idx: {i}, original token: {original_sequence[i]}, decoder token: {de_token}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"diff probs max: {max(abs(probs[i] - prob_de[i]))}, original sum error: {abs(sum(prob_de[i]) - 1.0)}, decoder sum error: {abs(sum(probs[i]) - 1.0)}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"original: {top_indices}, target_in_top5: {target_in_top5} decode: {top_indices_de}, \"\n",
    "            )\n",
    "            print(f\"target diff: {target_diff}\")\n",
    "            if original_sequence[i] != de_token:\n",
    "                import pdb\n",
    "                pdb.set_trace()\n",
    "        \n",
    "    return sequence_array_de_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "178b66f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_padded_bytes(filename: str, data: bytes, num_padded_bits: int, original_length: int):\n",
    "    \"\"\"\n",
    "    file format:\n",
    "    - first byte: number of padded bit\n",
    "    - second and third byte: original length (usually, llm context will not exceed 65535)\n",
    "    - subsequent bytes: actual bytes data\n",
    "\n",
    "    :param filename: output file name\n",
    "    :param data: bytes data to write\n",
    "    :param padding_bits: number of padded bits (must be between 0 and 7)\n",
    "    :param original_length: original length of the uncompressed data (in tokens)\n",
    "    \"\"\"\n",
    "\n",
    "    if not 0 <= num_padded_bits <= 7:\n",
    "        raise ValueError(\"num_padded_bits must be between 0 and 7.\")\n",
    "\n",
    "    if not 0 <= original_length <= 65535:\n",
    "        raise ValueError(\"original_length must be between 0 and 65535.\")\n",
    "\n",
    "    if not isinstance(data, bytes):\n",
    "        raise TypeError(\"data must be of bytes type.\")\n",
    "\n",
    "    with open(filename, 'wb') as f:\n",
    "        padding_byte = num_padded_bits.to_bytes(1, 'big')\n",
    "        f.write(padding_byte)\n",
    "        f.write(original_length.to_bytes(2, 'big'))\n",
    "        f.write(data)\n",
    "\n",
    "def read_padded_bytes(filename: str) -> tuple[bytes, int]:\n",
    "    \"\"\"\n",
    "    Read data and padding bits from a file.\n",
    "\n",
    "    :param filename: The name of the file to read.\n",
    "    :return: A tuple containing (bytes data, number of padded bits).\n",
    "             May raise an error if the file is empty or improperly formatted.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filename, 'rb') as f:\n",
    "        # the first byte indicates the number of padded bits\n",
    "        padding_byte = f.read(1)\n",
    "\n",
    "        # If the file is empty, f.read(1) will return an empty bytes object b''\n",
    "        if not padding_byte:\n",
    "            raise EOFError(\"File is empty or improperly formatted: unable to read padding bits byte.\")\n",
    "\n",
    "        original_length_bytes = f.read(2)\n",
    "        if not original_length_bytes:\n",
    "            raise EOFError(\"File is empty or improperly formatted: unable to read original length bytes.\")\n",
    "    \n",
    "        padding_bits = int.from_bytes(padding_byte, 'big')\n",
    "        original_length = int.from_bytes(original_length_bytes, 'big')\n",
    "\n",
    "        data = f.read()\n",
    "        \n",
    "        return data, padding_bits, original_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89a8cf92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'2G\\xd9[\\xa4d'\n",
      "1\n",
      "6\n",
      "b'2G\\xd9[\\xa4d'\n",
      "1\n",
      "6\n",
      "idx: 0, original token: 4285, decoder token: 4285\n",
      "diff probs max: 0.0, original sum error: 0.000357210636138916, decoder sum error: 0.000357210636138916\n",
      "original: [ 2462  1515    69  1040 76652], target_in_top5: False decode: [ 2462  1515    69  1040 76652], \n",
      "target diff: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c0/qh7w74pn7x5f5q81vn0wf7480000gn/T/ipykernel_61322/665979826.py:137: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequence_array_de_input = torch.tensor(sequence_array_de_input, dtype=torch.long, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 1, original token: 1467, decoder token: 1467\n",
      "diff probs max: 0.0, original sum error: 0.00028574466705322266, decoder sum error: 0.00028574466705322266\n",
      "original: [  11  323 3405 1616  714], target_in_top5: False decode: [  11  323 3405 1616  714], \n",
      "target diff: 0.0\n",
      "idx: 2, original token: 311, decoder token: 311\n",
      "diff probs max: 0.0, original sum error: 0.000306546688079834, decoder sum error: 0.000306546688079834\n",
      "original: [6440  311 1034 5980 8692], target_in_top5: True decode: [6440  311 1034 5980 8692], \n",
      "target diff: 0.0\n",
      "idx: 3, original token: 387, decoder token: 387\n",
      "diff probs max: 0.0, original sum error: 0.00027436017990112305, decoder sum error: 0.00027436017990112305\n",
      "original: [2168 1467 8806 9308 7699], target_in_top5: False decode: [2168 1467 8806 9308 7699], \n",
      "target diff: 0.0\n",
      "idx: 4, original token: 12510, decoder token: 12510\n",
      "diff probs max: 0.0, original sum error: 0.00024390220642089844, decoder sum error: 0.00024390220642089844\n",
      "original: [ 1349  2952  1483 12596  5326], target_in_top5: False decode: [ 1349  2952  1483 12596  5326], \n",
      "target diff: 0.0\n",
      "idx: 5, original token: 13, decoder token: 13\n",
      "diff probs max: 0.0, original sum error: 0.0002643465995788574, decoder sum error: 0.0002643465995788574\n",
      "original: [369  13 448 271 389], target_in_top5: True decode: [369  13 448 271 389], \n",
      "target diff: 0.0\n",
      "[19284  4285  1467   311   387 12510    13]\n",
      "tensor([[19284,  4285,  1467,   311,   387, 12510,    13]])\n",
      "Compression time: 0.49 seconds\n",
      "Decompression time: 0.95 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# model and tokenizer loading\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"pretrained/Qwen2.5-0.5B\", torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"pretrained/Qwen2.5-0.5B\", use_fast=False)\n",
    "llm.eval()\n",
    "\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# data\n",
    "sample_text = \"Super simple text to be tested.\"\n",
    "# sample_text = \"In a shocking finding, scientists discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\"\n",
    "#sample_text = r\"\"\"Greenhouse gas emissions from the burning of fossil fuels have pushed the acidity of the world's oceans past a safe threshold, scientists warn, threatening their ability to sustain shellfish and corals and help us in the fight against climate change. A new report says that ocean acidification is the latest \"planetary boundary\" to be crossed, a reference to a set of warning signs related to key planetary systems that keep the Earth safe for human civilization. Other planetary boundaries that have already been crossed — including dangerous levels of chemical pollution, the warming atmosphere and changes to the nutrient cycle — have already signalled threats to people.  \"Go outside of these boundaries and you first enter a danger zone, with higher risk of causing changes that would undermine that ability to support human life and human development,\" said Johan Rockström, director of the Potsdam Institute for Climate Impact Research, which is behind the Planetary Health Check report released on Wednesday. \"And once you are at the upper end of the uncertainty range ... you enter the red zone, the high-risk zone where most science agrees that we are very likely to depress buttons that will cause irreversible changes, basically committing ourselves to drifting away from livable conditions on Earth.\" Adding the oceans to the planetary boundaries list is a major concern because of the billions of people who depend on them. Continuing ocean acidification could not only destroy fisheries that people rely on for food but reduce the ability of the ocean to absorb carbon dioxide and moderate global warming. As humans burn fossil fuels and pump carbon dioxide into the atmosphere, it's estimated that the ocean is absorbing more than a quarter of that CO2.  \"Just like when we add carbon dioxide to Coke or soda, that makes the soft drink more acidic,\" said Christopher Harley, a professor who studies climate change and the ocean at the University of British Columbia.  But when CO2 is absorbed, the chemical process effectively lowers the availability of a mineral that certain marine life — from shellfish to coral — need to develop their bodies. \"It makes it harder to build shells — and you need to add shell if you want to grow bigger,\" Harley explained, comparing it to the construction of a house.  \"All of a sudden, the building materials become more costly. You're either going to build smaller homes or not as many.\" \"\"\"\n",
    "\n",
    "# work flow\n",
    "compression_start_time = time.time()\n",
    "\n",
    "tokenized = tokenizer(sample_text, return_tensors=\"pt\")\n",
    "\n",
    "metric = Metric()\n",
    "with torch.inference_mode():\n",
    "    # we don't need the last token's logits\n",
    "    logits = (\n",
    "        llm(tokenized[\"input_ids\"], use_cache=False).logits[:, :-1].to(torch.float32)\n",
    "    )\n",
    "compressed_bytes, num_padded_bits, start_symbol, sequence_array, pd, probs = compress(\n",
    "    tokenized[\"input_ids\"], logits, metric\n",
    ")\n",
    "\n",
    "compression_end_time = time.time()\n",
    "\n",
    "print(compressed_bytes)\n",
    "print(num_padded_bits)\n",
    "original_length = tokenized[\"input_ids\"].shape[1] - 1\n",
    "print(original_length)\n",
    "write_padded_bytes(\"compressed.bin\", compressed_bytes, num_padded_bits, original_length)\n",
    "compressed_bytes, num_padded_bits, original_length = read_padded_bytes(\"compressed.bin\")\n",
    "print(compressed_bytes)\n",
    "print(num_padded_bits)\n",
    "print(original_length)\n",
    "\n",
    "decompression_start_time = time.time()\n",
    "\n",
    "decompressed = decode(\n",
    "    compressed_bytes,\n",
    "    num_padded_bits,\n",
    "    llm,\n",
    "    start_symbol,\n",
    "    device,\n",
    "    original_length,\n",
    "    sequence_array,\n",
    "    pd,\n",
    "    probs,\n",
    "    do_test=True,\n",
    ")\n",
    "\n",
    "decompression_end_time = time.time()\n",
    "\n",
    "print(tokenized[\"input_ids\"].squeeze(0).numpy())\n",
    "print(decompressed)\n",
    "\n",
    "print(f\"Compression time: {compression_end_time - compression_start_time:.2f} seconds\")\n",
    "print(f\"Decompression time: {decompression_end_time - decompression_start_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
